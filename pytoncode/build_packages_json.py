# -*- coding: utf-8 -*-
"""
ÙŠØ¨Ù†ÙŠ Ù…Ù„Ù JSON Ø¨Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ø¹ØªÙ…Ø§Ø¯Ø§Ù‹ Ø¹Ù„Ù‰:
- Ø­Ø¯ÙˆØ¯.docx  â†’ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø­Ø²Ù…/Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª/Ø§Ù„Ø¨ÙˆØªØ§Øª + Ù†Øµ Ø§Ù„Ø­Ø¯ÙˆØ¯
- Ù†Ø¨Ø°Ø©.docx  â†’ Ù†Øµ Ø§Ù„Ù†Ø¨Ø°Ø© Ù„ÙƒÙ„ Ø¨ÙˆØª (ÙŠØ¯Ø¹Ù… @@@Ø¹Ù†ÙˆØ§Ù† Ø«Ù… Ø§Ù„Ù†ØµØŒ Ø£Ùˆ "Ø¹Ù†ÙˆØ§Ù†":"Ø§Ù„Ù†Øµ")
- Ù…Ø«Ø§Ù„.docx  â†’ Ù†Øµ Ø§Ù„Ù…Ø«Ø§Ù„ Ù„ÙƒÙ„ Ø¨ÙˆØª (ÙŠØ¯Ø¹Ù… "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†":"Ø§Ù„Ù†Øµ" ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø³Ø·Ø±ØŒ
               Ø£Ùˆ ÙƒØ³Ø± Ø³Ø·Ø± Ù…Ø¹ 'Ø§Ù„ÙˆØµÙ (Ù…Ø«Ø§Ù„): "Ø§Ù„Ù†Øµ"')
- Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©.docx â†’ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ 4O/5 (ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ Ø§Ù„Ø®Ù„Ø§ÙŠØ§)

Ø§Ø­ÙØ¸Ù‡ Ø¨Ø§Ø³Ù… build_packages_json.py Ø«Ù… Ø´ØºÙ‘Ù„Ù‡.
"""

import json
import os
import re
import difflib
from collections import OrderedDict, defaultdict
from docx import Document
from docx.oxml.ns import qn
from pathlib import Path

# ======== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ========
# Ø¥Ù† ÙƒØ§Ù†Øª Ù…Ù„ÙØ§ØªÙƒ ÙÙŠ /mnt/data ÙƒÙ…Ø§ ÙÙŠ Ø¬Ù„Ø³Ø© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠØ©ØŒ Ø§ØªØ±Ùƒ BASE ÙƒÙ…Ø§ Ù‡Ùˆ.
BASE = Path(__file__).resolve().parent
HUDUD_PATH = os.path.join(BASE, "Ø­Ø¯ÙˆØ¯.docx")
NOBTHA_PATH = os.path.join(BASE, "Ù†Ø¨Ø°Ø©.docx")
MITHAL_PATH = os.path.join(BASE, "Ù…Ø«Ø§Ù„.docx")
# LINKS_PATH  = os.path.join(BASE, "Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©.docx")
LINKS_PATH  = os.path.join(BASE, "old.docx")
OUTPUT_JSON = os.path.join(BASE, "output.json")


# ======== Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø© ========
AR_QUOTE_CHARS = 'Â«Â»â€œâ€â€â€Ÿâ€šâ€›'
EN_QUOTE_CHARS = '"\''
PUNCT_TO_STRIP = 'ï¼š:Ø›ØŒ,'

def normalize_title(s: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ø³Ù… Ø§Ù„Ø¨ÙˆØª/Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø¥Ø²Ø§Ù„Ø© Ù…Ø­Ø§Ø±Ù Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ© ÙˆØ§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª."""
    if not s:
        return ""
    s = s.strip()
    s = re.sub(r'^\s*#\s*', '', s)  # Ø£Ø²Ù„ Ø¨Ø§Ø¯Ø¦Ø© #
    s = s.strip(AR_QUOTE_CHARS + EN_QUOTE_CHARS + PUNCT_TO_STRIP)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def norm_for_match(s: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ø£Ø®Ù Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠØ©."""
    if not s: return ""
    s = re.sub(r'[\u200f\u200e]', '', s)  # Ù…Ø­Ø§Ø±Ù Ø§ØªØ¬Ø§Ù‡ÙŠØ©
    s = s.replace("Ù€", "")                # ÙƒÙØ´Ù’Ø¯ÙØ© Ù…Ù…ØªØ¯Ø©
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def best_match_title(text, known_titles, cutoff=0.88):
    """ÙŠØ¹ÙŠØ¯ Ø£ÙØ¶Ù„ Ø¹Ù†ÙˆØ§Ù† Ù…Ø¹Ø±ÙˆÙ ÙŠØ¸Ù‡Ø± Ø¯Ø§Ø®Ù„ Ø§Ù„Ù†Øµ Ø£Ùˆ Ø£Ù‚Ø±Ø¨Ù‡ ØªÙ‚Ø±ÙŠØ¨ÙŠØ§Ù‹."""
    text_n = norm_for_match(text)
    # 1) Ø§Ø­ØªÙˆØ§Ø¡ Ù…Ø¨Ø§Ø´Ø± (Ù†Ø®ØªØ§Ø± Ø§Ù„Ø£Ø·ÙˆÙ„)
    cands = [t for t in known_titles if t and norm_for_match(t) in text_n]
    if cands:
        return max(cands, key=len)
    # 2) Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ‚Ø±ÙŠØ¨ÙŠØ©
    matches = difflib.get_close_matches(text_n, [norm_for_match(t) for t in known_titles], n=1, cutoff=cutoff)
    if matches:
        # Ø£Ø¹Ø¯ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø£ØµÙ„ÙŠ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ Ù„Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…ÙØ·Ø¨Ù‘Ø¹Ø©
        match_n = matches[0]
        for t in known_titles:
            if norm_for_match(t) == match_n:
                return t
    return None

def read_docx_lines(path: str):
    """Ù‚Ø±Ø§Ø¡Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙÙ‚Ø±Ø§Øª ØºÙŠØ± Ø§Ù„ÙØ§Ø±ØºØ© ÙƒØ³Ø·ÙˆØ± Ù†ØµÙŠØ©."""
    if not os.path.exists(path):
        return []
    doc = Document(path)
    lines = []
    for p in doc.paragraphs:
        t = p.text.strip()
        if t:
            lines.append(t)
    return lines

def is_package_line(line: str):
    # Ù…Ø«Ø§Ù„: "Ø¨Ø§Ù‚Ø© ...."
    return bool(re.match(r'^\s*Ø¨Ø§Ù‚Ø©\s+', line))

def is_category_line(line: str):
    # ÙŠØ¯Ø¹Ù…: "ØªØµÙ†ÙŠÙ ..." Ø£Ùˆ "Ù†Ù…Ø§Ø°Ø¬ ..."
    return bool(re.match(r'^\s*(ØªØµÙ†ÙŠÙ|Ù†Ù…Ø§Ø°Ø¬)\s+', line))

def is_bot_header_line(line: str):
    # Ø³Ø·Ø± ÙŠØ¨Ø¯Ø£ Ø¨Ù€ #Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„Ø¨ÙˆØª
    return bool(re.match(r'^\s*#', line))


# ======== ØªØ­Ù„ÙŠÙ„ Ø­Ø¯ÙˆØ¯.docx â†’ (Ø­ÙØ²Ù… â† ØªØµÙ†ÙŠÙØ§Øª â† Ø¨ÙˆØªØ§Øª + Ù†Øµ Ø§Ù„Ø­Ø¯ÙˆØ¯) ========
def parse_hudud(lines):
    """
    ÙŠÙØ¹ÙŠØ¯:
    OrderedDict {
        package_name: {'categories': OrderedDict {
            category_name: {'bots': OrderedDict {
                bot_title: {'hudud': '...'}
            }}
        }}
    }
    """
    packages = OrderedDict()
    current_package = None
    current_category = None
    current_bot = None
    buffer = []

    def flush_bot():
        nonlocal buffer, current_package, current_category, current_bot
        if current_package and current_category and current_bot:
            text = "\n".join(buffer).strip()
            packages.setdefault(current_package, {'categories': OrderedDict()})
            cats = packages[current_package]['categories']
            cats.setdefault(current_category, {'bots': OrderedDict()})
            bots = cats[current_category]['bots']
            bots.setdefault(current_bot, {})
            bots[current_bot]['hudud'] = text
        buffer = []

    for line in lines:
        if is_package_line(line):
            flush_bot()
            current_package = normalize_title(line)
            current_category = None
            current_bot = None
        elif is_category_line(line):
            flush_bot()
            current_category = normalize_title(line)
            current_bot = None
        elif is_bot_header_line(line):
            flush_bot()
            current_bot = normalize_title(line)
        else:
            if current_bot:
                buffer.append(line)
    flush_bot()
    return packages


# ======== ØªØ­Ù„ÙŠÙ„ Ù†Ø¨Ø°Ø©.docx â†’ {bot_title: 'Ù†Ø¨Ø°Ø©...'} ========
def parse_nobtha(lines):
    """
    ÙŠØ¯Ø¹Ù… Ø´ÙƒÙ„ÙŠÙ†:
      1) @@@Ø¹Ù†ÙˆØ§Ù†_Ø§Ù„Ø¨ÙˆØª
         ...Ø§Ù„Ù†Øµ Ø­ØªÙ‰ @@@ Ø§Ù„ØªØ§Ù„ÙŠ
      2) "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†":"Ø§Ù„Ù†Øµ"  (Ø¨Ù†ÙØ³ Ø§Ù„Ø³Ø·Ø± Ø£Ùˆ Ø¨ØµÙŠØºØ© ÙƒØ³Ø·Ø±ÙŠÙ† Ù…Ø¹ "Ø§Ù„ÙˆØµÙ (Ù†Ø¨Ø°Ø©):")
    """
    # Ø£ÙˆÙ„Ø§Ù‹: Ù…Ø­Ø§ÙˆÙ„Ø© Ù†Ù…Ø· Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ (Ù…ÙÙŠØ¯ Ù„Ùˆ Ø§Ù„Ù…Ù„Ù Ù…Ù†Ø¸Ù‘Ù… Ø¨Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©)
    text = "\n".join(lines)
    pair_pattern = re.compile(
        r'[\"â€œ](.+?)[\"â€]\s*[:ï¼š]\s*'                 # "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†":
        r'(?:\r?\n\s*)?'                              # ÙƒØ³Ø± Ø³Ø·Ø± Ø§Ø®ØªÙŠØ§Ø±ÙŠ
        r'(?:Ø§Ù„ÙˆØµÙ\s*\(?\s*Ù†Ø¨Ø°Ø©\s*\)?\s*[:ï¼š]\s*)?'   # "Ø§Ù„ÙˆØµÙ (Ù†Ø¨Ø°Ø©):" Ø§Ø®ØªÙŠØ§Ø±ÙŠ
        r'[\"â€œ](.+?)[\"â€]',                           # "Ø§Ù„Ù†Øµ"
        flags=re.MULTILINE
    )
    result = {}
    for m in pair_pattern.finditer(text):
        title = normalize_title(m.group(1))
        desc  = m.group(2).strip()
        result[title] = desc

    # Ø«Ø§Ù†ÙŠØ§Ù‹: Ø¯Ø¹Ù… Ø£Ø³Ù„ÙˆØ¨ @@@
    current_title = None
    buffer = []
    def flush():
        nonlocal current_title, buffer
        if current_title and buffer and current_title not in result:
            result[current_title] = "\n".join(buffer).strip()
        buffer = []

    for line in lines:
        if line.strip().startswith('@@@'):
            flush()
            current_title = normalize_title(line.replace('@@@', '', 1))
        else:
            if current_title:
                buffer.append(line)
    flush()
    return result


# ======== ØªØ­Ù„ÙŠÙ„ Ù…Ø«Ø§Ù„.docx â†’ {bot_title: 'Ù…Ø«Ø§Ù„...'} ========
def parse_mithal(lines):
    """
    ÙŠÙ„ØªÙ‚Ø· Ø§Ù„ØµÙŠØº:
    1) "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†": "Ø§Ù„Ù†Øµ"
    2) "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†":
       Ø§Ù„ÙˆØµÙ (Ù…Ø«Ø§Ù„): "Ø§Ù„Ù†Øµ"
    """
    text = "\n".join(lines)
    pair_pattern = re.compile(
        r'[\"â€œ](.+?)[\"â€]\s*[:ï¼š]\s*'               # "Ø§Ù„Ø¹Ù†ÙˆØ§Ù†":
        r'(?:\r?\n\s*)?'                            # ÙƒØ³Ø± Ø³Ø·Ø± Ø§Ø®ØªÙŠØ§Ø±ÙŠ
        r'(?:Ø§Ù„ÙˆØµÙ\s*\(?\s*Ù…Ø«Ø§Ù„\s*\)?\s*[:ï¼š]\s*)?' # "Ø§Ù„ÙˆØµÙ (Ù…Ø«Ø§Ù„):" Ø§Ø®ØªÙŠØ§Ø±ÙŠ
        r'[\"â€œ](.+?)[\"â€]',                         # "Ø§Ù„Ù†Øµ"
        flags=re.MULTILINE
    )

    result = {}
    for m in pair_pattern.finditer(text):
        title = normalize_title(m.group(1))
        desc  = m.group(2).strip()
        result[title] = desc
    return result


# ======== ØªØ­Ù„ÙŠÙ„ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©.docx (ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø·) ========
def parse_links(path, known_titles):
    """
    ÙŠÙ‚Ø±Ø£ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ù† Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„ÙÙ‚Ø±Ø§ØªØŒ ÙˆÙŠÙØ±Ø¬Ø¹:
      { 'Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¨ÙˆØª': {'4O': url_or_empty, '5': url_or_empty}, ... }
    """
    if not os.path.exists(path):
        return {}

    doc = Document(path)
    known_titles = list(known_titles)  # Ù„Ø¶Ù…Ø§Ù† Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ÙÙ‡Ø±Ø³Ø©

    def clean_title_in_cell(s: str) -> str:
        if not s: return ""
        s = s.replace("ğŸ”—", "").strip()
        s = re.sub(r'\s*[â€“\-â€”]\s*Ù†Ù…ÙˆØ°Ø¬\s*[45](?:o|O)?\s*$', '', s).strip()
        s = re.sub(r'\s+', ' ', s).strip()
        return s

    def detect_model_from_text(txt: str):
        m = re.search(r'Ù†Ù…ÙˆØ°Ø¬\s*([45](?:o|O)?)', txt, re.IGNORECASE)
        if not m: return None
        g = m.group(1)
        if g in ('5', 'Ù¥'): return '5'
        return '4O'

    def detect_model_from_url(url: str):
        u = url.lower()
        if any(k in u for k in ['mod-5', '/5', 'gpt-5', 'model-5']):
            return '5'
        if any(k in u for k in ['mod-4o', 'gpt-4o', '4o', 'model-4o', 'mod-4']):
            return '4O'
        return None

    def hyperlinks_in_paragraph(p):
        # ÙŠØ¹ÙŠØ¯ Ù‚Ø§Ø¦Ù…Ø© r:id Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„ÙÙ‚Ø±Ø©
        return [hl.get(qn('r:id'))
                for hl in p._p.findall('.//{http://schemas.openxmlformats.org/wordprocessingml/2006/main}hyperlink')
                if hl.get(qn('r:id'))]

    result = defaultdict(lambda: {'4O': '', '5': ''})

    # --- 1) Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„: Ù†Ø®ØªØ§Ø± Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙ Ø«Ù… Ù†Ø±Ø¨Ø· ÙƒÙ„ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙ Ø¨Ù‡ ---
    for table in doc.tables:
        for row in table.rows:
            # Ø§Ø¬Ù…Ø¹ Ù†ØµÙˆØµ Ø§Ù„ØµÙ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø£ÙØ¶Ù„
            row_text = " | ".join(c.text.strip() for c in row.cells if c.text.strip())
            row_title = best_match_title(clean_title_in_cell(row_text), known_titles)
            # Ø§Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø§Ù„ØµÙ
            for cell in row.cells:
                cell_text = cell.text.strip()
                # Ø¥Ù† Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø¹Ù†ÙˆØ§Ù† Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµÙØŒ Ø¬Ø±Ù‘Ø¨ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ù„ÙŠØ©
                cell_title = row_title or best_match_title(clean_title_in_cell(cell_text), known_titles)

                rids = []
                for p in cell.paragraphs:
                    rids.extend(hyperlinks_in_paragraph(p))

                for rId in rids:
                    rel = doc.part.rels.get(rId)
                    if not rel:
                        continue
                    url = rel.target_ref
                    model = detect_model_from_text(cell_text) or detect_model_from_url(url)
                    # Ù„Ùˆ Ù„Ù… Ù†Ø¹Ø±Ù Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ØŒ Ø¹ÙŠÙ‘Ù†Ù‡ Ù…Ø¤Ù‚ØªÙ‹Ø§ 4O Ù„ØªØ¹Ø¨Ø¦Ø© Ø®Ø§Ù†Ø© ÙˆØ§Ø­Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„
                    if not model:
                        model = '4O'
                    title_key = cell_title
                    if title_key:
                        result[title_key][model] = url

    # --- 2) Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ø­Ø±Ø© Ø®Ø§Ø±Ø¬ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ (Ø´Ø¨ÙƒØ© Ø£Ù…Ø§Ù†) ---
    for p in doc.paragraphs:
        rids = hyperlinks_in_paragraph(p)
        if not rids:
            continue
        t = p.text.strip()
        title_in_p = best_match_title(clean_title_in_cell(t), known_titles)
        for rId in rids:
            rel = doc.part.rels.get(rId)
            if not rel:
                continue
            url = rel.target_ref
            model = detect_model_from_text(t) or detect_model_from_url(url) or '4O'
            if title_in_p:
                result[title_in_p][model] = url

    return dict(result)


# ======== Ø¨Ù†Ø§Ø¡ JSON Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ========
def build_json_from_docs():
    hudud_lines = read_docx_lines(HUDUD_PATH)
    nobtha_lines = read_docx_lines(NOBTHA_PATH)
    mithal_lines = read_docx_lines(MITHAL_PATH)

    packages = parse_hudud(hudud_lines)
    nobtha_map = parse_nobtha(nobtha_lines)
    mithal_map = parse_mithal(mithal_lines)

    # Ù‚Ø§Ø¦Ù…Ø© Ø¨ÙƒÙ„ Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨ÙˆØªØ§Øª Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ© Ù…Ù† Ø­Ø¯ÙˆØ¯.docx
    known_bot_titles = []
    for pkg in packages.values():
        for cat in pkg['categories'].values():
            known_bot_titles.extend(list(cat['bots'].keys()))

    links_map = parse_links(LINKS_PATH, known_bot_titles)

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ±ÙƒÙŠØ¨ Ø¥Ù„Ù‰ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
    out = {"packages": []}
    package_id_counter = 1

    for package_name, pkg_obj in packages.items():
        package_entry = {
            "package": package_name,
            "packageId": package_id_counter,
            "categories": []
        }
        package_id_counter += 1

        for category_name, cat_obj in pkg_obj['categories'].items():
            category_entry = {
                "category": category_name,
                "bots": []
            }
            for bot_title, bot_obj in cat_obj['bots'].items():
                bot_entry = {
                    "botTitle": bot_title,
                    "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬": {
                        "4O": links_map.get(bot_title, {}).get("4O", ""),
                        "5":  links_map.get(bot_title, {}).get("5", "")
                    },
                    "Ù†Ø¨Ø°Ø©": nobtha_map.get(bot_title, ""),
                    "Ø­Ø¯ÙˆØ¯": bot_obj.get("hudud", ""),
                    "Ù…Ø«Ø§Ù„": mithal_map.get(bot_title, "")
                }
                category_entry["bots"].append(bot_entry)

            package_entry["categories"].append(category_entry)
        out["packages"].append(package_entry)

    # Ø­ÙØ¸ JSON
    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(out, f, ensure_ascii=False, indent=2)

    return out


if __name__ == "__main__":
    data = build_json_from_docs()
    print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù: {OUTPUT_JSON}")
    # Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹
    for p in data["packages"]:
        print(f"- {p['package']} (id={p['packageId']}): {len(p['categories'])} ØªØµÙ†ÙŠÙ")
        for c in p["categories"]:
            print(f"  â€¢ {c['category']}: {len(c['bots'])} Ø¨ÙˆØª")
